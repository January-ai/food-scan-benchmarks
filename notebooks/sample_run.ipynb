{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3f5fa9b",
   "metadata": {},
   "source": [
    "# Food Scan Benchmark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f3b78b",
   "metadata": {},
   "source": [
    "This notebook provides a complete, end-to-end workflow for benchmarking Multimodal Large Language Models (MLLMs) on January's food image dataset (JFID).\n",
    "\n",
    "**The process is as follows:**\n",
    "\n",
    "1.  **Setup:** Install dependencies and configure API keys.\n",
    "2.  **Define Components:** Set up Pydantic schemas, the dataset loader, the model wrapper, and evaluation metrics.\n",
    "3.  **Run Evaluation:** Loop through the dataset, send images to a chosen MLLM, and collect predictions.\n",
    "4.  **Analyze Results:** Calculate metrics and summarize the model's performance.\n",
    "\n",
    "The dataset is downloaded from a public S3 bucket and cached locally.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0dd8db",
   "metadata": {},
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85827fa",
   "metadata": {},
   "source": [
    "Add your API keys to a `.env` file in the same directory as this notebook:\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY=\"sk-...\"\n",
    "GEMINI_API_KEY=\"...\"\n",
    "JANUARY_API_ENDPOINT=\"...\"\n",
    "JANUARY_API_UUID=\"...\"\n",
    "JANUARY_API_TOKEN=\"...\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374df0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install packages\n",
    "%pip install --upgrade litellm boto3 pandas tqdm python-dotenv pydantic tabulate openai scikit-learn scipy plotly openpyxl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47337d0",
   "metadata": {},
   "source": [
    "## Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679a9d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "import openai\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "import hashlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import boto3\n",
    "from botocore import UNSIGNED\n",
    "from botocore.client import Config\n",
    "import tarfile\n",
    "import litellm\n",
    "from litellm.exceptions import APIError\n",
    "from typing import Optional, Dict\n",
    "import json\n",
    "import base64\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "import asyncio\n",
    "import time\n",
    "from typing import Union, Tuple\n",
    "from difflib import SequenceMatcher\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import httpx\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0107115d",
   "metadata": {},
   "source": [
    "## Core Components\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4789e0b5",
   "metadata": {},
   "source": [
    "### Schema Definition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570dc178",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ingredient(BaseModel):\n",
    "    name: str = Field(description=\"Name of the ingredient, e.g., 'scrambled eggs'\")\n",
    "    quantity: float = Field(description=\"Numerical quantity of the ingredient\")\n",
    "    unit: str = Field(description=\"Unit of measurement, e.g., 'cup', 'slice', 'g'\")\n",
    "    calories: float = Field(description=\"Estimated calories for this ingredient\")\n",
    "    carbs: float = Field(\n",
    "        description=\"Estimated grams of carbohydrates for this ingredient\"\n",
    "    )\n",
    "    protein: float = Field(description=\"Estimated grams of protein for this ingredient\")\n",
    "    fat: float = Field(description=\"Estimated grams of fat for this ingredient\")\n",
    "\n",
    "\n",
    "class TotalMacros(BaseModel):\n",
    "    calories: float = Field(description=\"Total estimated calories for the entire meal\")\n",
    "    carbs: float = Field(\n",
    "        description=\"Total estimated grams of carbohydrates for the entire meal\"\n",
    "    )\n",
    "    protein: float = Field(\n",
    "        description=\"Total estimated grams of protein for the entire meal\"\n",
    "    )\n",
    "    fat: float = Field(description=\"Total estimated grams of fat for the entire meal\")\n",
    "\n",
    "\n",
    "class FoodAnalysis(BaseModel):\n",
    "    meal_name: str = Field(\n",
    "        description=\"A descriptive name for the meal, e.g., 'Breakfast Platter'\"\n",
    "    )\n",
    "    ingredients: List[Ingredient] = Field(\n",
    "        description=\"A list of all identified ingredients and their nutritional information\"\n",
    "    )\n",
    "    total_macros: TotalMacros = Field(\n",
    "        description=\"The sum of macros for all ingredients\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4438935",
   "metadata": {},
   "source": [
    "### Dataset Class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe960109",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FoodScanDataset:\n",
    "    \"\"\"Handles downloading, caching, and loading the food dataset.\"\"\"\n",
    "\n",
    "    _S3_BUCKET = \"january-food-image-dataset-public\"\n",
    "    _S3_KEY = \"food-scan-benchmark-dataset.tar.gz\"\n",
    "\n",
    "    def __init__(self, root: Path):\n",
    "        self.root = root.expanduser()\n",
    "        self.img_dir = self.root / \"food-scan-benchmark-dataset\" / \"fsb_images\"\n",
    "        self.csv_path = (\n",
    "            self.root / \"food-scan-benchmark-dataset\" / \"food_scan_bench_v1.csv\"\n",
    "        )\n",
    "\n",
    "        if not self.csv_path.exists():\n",
    "            self._download_and_extract()\n",
    "\n",
    "        self.df = pd.read_csv(self.csv_path)\n",
    "\n",
    "    def _download_and_extract(self):\n",
    "        print(f\"Dataset not found in {self.root}. Downloading from S3...\")\n",
    "        self.root.mkdir(parents=True, exist_ok=True)\n",
    "        local_archive = self.root / \"fsb.tar.gz\"\n",
    "\n",
    "        s3 = boto3.client(\n",
    "            \"s3\",\n",
    "            config=Config(signature_version=UNSIGNED),\n",
    "        )\n",
    "        with open(local_archive, \"wb\") as f:\n",
    "            s3.download_fileobj(self._S3_BUCKET, self._S3_KEY, f)\n",
    "\n",
    "        print(\"Download complete. Extracting...\")\n",
    "        with tarfile.open(local_archive) as tar:\n",
    "            tar.extractall(path=self.root)\n",
    "        local_archive.unlink()\n",
    "        print(\"Extraction complete.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        row = self.df.iloc[idx]\n",
    "        img_path = self.img_dir / row.image_filename\n",
    "\n",
    "        try:\n",
    "            ingredients = ast.literal_eval(row.ingredients_list)\n",
    "        except (ValueError, SyntaxError):\n",
    "            ingredients = []\n",
    "\n",
    "        return {\n",
    "            \"image_id\": row.image_id,\n",
    "            \"image_path\": img_path,\n",
    "            \"gt\": {\n",
    "                \"meal_name\": row.meal_name,\n",
    "                \"ingredients\": ingredients,\n",
    "                \"macros\": {\n",
    "                    \"calories\": row.total_calories,\n",
    "                    \"carbs\": row.total_carbs,\n",
    "                    \"protein\": row.total_protein,\n",
    "                    \"fat\": row.total_fat,\n",
    "                },\n",
    "            },\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23cbb92b",
   "metadata": {},
   "source": [
    "### Model config and helpers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7664d328",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_COSTS = {\n",
    "    \"january/food-vision-v1\": {\n",
    "        \"input\": 0.0,  # Not used, cost is per-image\n",
    "        \"output\": 0.0,\n",
    "        \"display_name\": \"January AI\",\n",
    "    },\n",
    "    \"gpt-4.1\": {\n",
    "        \"input\": 2.00,\n",
    "        \"output\": 8.00,\n",
    "        \"display_name\": \"gpt-4.1\",\n",
    "    },\n",
    "    \"gpt-4o\": {\n",
    "        \"input\": 2.50,\n",
    "        \"output\": 10.00,\n",
    "        \"display_name\": \"gpt-4o\",\n",
    "    },\n",
    "    \"gpt-4o-mini\": {\n",
    "        \"input\": 0.15,\n",
    "        \"output\": 0.60,\n",
    "        \"display_name\": \"gpt-4o-mini\",\n",
    "    },\n",
    "    \"gemini/gemini-2.5-flash-preview-05-20\": {\n",
    "        \"input\": 0.15,\n",
    "        \"output\": 0.60,\n",
    "        \"display_name\": \"gemini-2.5-flash\",\n",
    "    },\n",
    "    \"gemini/gemini-2.5-pro-preview-06-05\": {\n",
    "        \"input\": 1.25,\n",
    "        \"output\": 10.00,\n",
    "        \"display_name\": \"gemini-2.5-pro\",\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "def img2b64(path: Path) -> str:\n",
    "    \"\"\"Converts an image file to a base64 encoded string for API calls.\"\"\"\n",
    "    encoded = base64.b64encode(path.read_bytes()).decode()\n",
    "    return f\"data:image/jpeg;base64,{encoded}\"\n",
    "\n",
    "\n",
    "def calculate_cost(model_name: str, input_tokens: int, output_tokens: int) -> float:\n",
    "    \"\"\"Calculate the cost for a model based on token usage.\"\"\"\n",
    "    if model_name not in MODEL_COSTS:\n",
    "        return 0.0\n",
    "\n",
    "    costs = MODEL_COSTS[model_name]\n",
    "    input_cost = (input_tokens / 1_000_000) * costs[\"input\"]\n",
    "    output_cost = (output_tokens / 1_000_000) * costs[\"output\"]\n",
    "    return round(input_cost + output_cost, 6)\n",
    "\n",
    "\n",
    "def get_display_name(model_name: str) -> str:\n",
    "    \"\"\"Return the user-friendly model label (falls back to raw id).\"\"\"\n",
    "    return MODEL_COSTS.get(model_name, {}).get(\"display_name\", model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0756e7",
   "metadata": {},
   "source": [
    "### Model Wrappers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0490690c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LiteModel:\n",
    "    \"\"\"A robust wrapper around any LiteLLM-supported vision model with prompt engineering options.\"\"\"\n",
    "\n",
    "    PROMPT_VARIANTS = {\n",
    "        \"detailed\": {\n",
    "            \"suffix\": \"d\",\n",
    "            \"prompt\": \"You are an expert nutritionist with 20 years of experience. Analyze this food image very carefully and provide the most accurate breakdown possible. Consider portion sizes, cooking methods, and hidden ingredients.\",\n",
    "        },\n",
    "        \"step_by_step\": {\n",
    "            \"suffix\": \"s\",\n",
    "            \"prompt\": \"You are an expert nutritionist. Please analyze this image step by step: 1) First identify all visible food items, 2) Estimate portion sizes, 3) Calculate nutritional content for each item, 4) Sum the totals. Be precise and systematic.\",\n",
    "        },\n",
    "        \"conservative\": {\n",
    "            \"suffix\": \"c\",\n",
    "            \"prompt\": \"You are a conservative nutritionist who prefers to underestimate rather than overestimate. Analyze this food image and provide a realistic, slightly conservative nutritional breakdown.\",\n",
    "        },\n",
    "        \"confident\": {\n",
    "            \"suffix\": \"f\",\n",
    "            \"prompt\": \"You are a highly confident nutritionist. Analyze this food image and provide your best estimate of the nutritional content. Trust your expertise.\",\n",
    "        },\n",
    "    }\n",
    "\n",
    "    def __init__(self, model_name: str, prompt_variant=\"detailed\", **litellm_kwargs):\n",
    "        self.model_name = model_name\n",
    "        self.prompt_variant = prompt_variant\n",
    "        self.kwargs = litellm_kwargs\n",
    "\n",
    "        cfg = self.PROMPT_VARIANTS.get(prompt_variant, self.PROMPT_VARIANTS[\"detailed\"])\n",
    "        self.system_prompt = cfg[\"prompt\"]\n",
    "        self.prompt_suffix = cfg[\"suffix\"]\n",
    "\n",
    "    async def analyse(self, img_path: Path) -> Tuple[Optional[dict], Optional[str]]:\n",
    "        \"\"\"\n",
    "        Analyzes an image and returns a structured dict with cost info, or None and an error message on failure.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[Optional[dict], Optional[str]]: A tuple of (result, error_message).\n",
    "                                                  On success, result is a dict and error_message is None.\n",
    "                                                  On failure, result is None and error_message is a string.\n",
    "        \"\"\"\n",
    "        b64_img = img2b64(img_path)\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": self.system_prompt},\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": \"Analyze this food image and provide a detailed nutritional breakdown. Include: meal name, all ingredients with quantities/units, and complete macro information (calories, carbs, protein, fat) for each ingredient and the total meal.\",\n",
    "                    },\n",
    "                    {\"type\": \"image_url\", \"image_url\": {\"url\": b64_img}},\n",
    "                ],\n",
    "            },\n",
    "        ]\n",
    "\n",
    "        try:\n",
    "            resp = await litellm.acompletion(\n",
    "                model=self.model_name,\n",
    "                messages=messages,\n",
    "                response_format=FoodAnalysis,\n",
    "                temperature=0.0,\n",
    "                **self.kwargs,\n",
    "            )\n",
    "            raw = resp.choices[0].message.content.strip()\n",
    "            data = json.loads(raw)\n",
    "\n",
    "            usage = resp.usage\n",
    "            input_tokens = usage.prompt_tokens if usage else 0\n",
    "            output_tokens = usage.completion_tokens if usage else 0\n",
    "            cost = calculate_cost(self.model_name, input_tokens, output_tokens)\n",
    "\n",
    "            result = FoodAnalysis(**data).model_dump()\n",
    "            result[\"cost_usd\"] = cost\n",
    "            result[\"prompt_variant\"] = self.prompt_variant\n",
    "\n",
    "            return result, None\n",
    "\n",
    "        except APIError as e:\n",
    "            error_msg = f\"API Error: {e}\"\n",
    "            print(f\"{error_msg} for {img_path.name}\")\n",
    "            return None, error_msg\n",
    "        except Exception as e:\n",
    "            error_msg = f\"Unexpected Error: {e}\"\n",
    "            print(f\"{error_msg} for {img_path.name}\")\n",
    "            return None, error_msg\n",
    "\n",
    "\n",
    "def pretty_label(full_model_name: str) -> str:\n",
    "    if full_model_name == \"january/food-vision-v1\":\n",
    "        return get_display_name(full_model_name)\n",
    "\n",
    "    for variant, meta in LiteModel.PROMPT_VARIANTS.items():\n",
    "        postfix = f\"_{variant}\"\n",
    "        if full_model_name.endswith(postfix):\n",
    "            base = full_model_name[: -len(postfix)]\n",
    "            return f\"{get_display_name(base)}_{meta['suffix']}\"\n",
    "\n",
    "    if \"_\" in full_model_name:\n",
    "        base, variant = full_model_name.rsplit(\"_\", 1)\n",
    "        suffix = LiteModel.PROMPT_VARIANTS.get(variant, {}).get(\n",
    "            \"suffix\", variant[0].lower()\n",
    "        )\n",
    "        return f\"{get_display_name(base)}_{suffix}\"\n",
    "\n",
    "    return get_display_name(full_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce51e3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JanuaryAIModel:\n",
    "    \"\"\"A wrapper for the proprietary January AI food vision API.\"\"\"\n",
    "\n",
    "    COST_PER_IMAGE = 0.01\n",
    "\n",
    "    def __init__(self):\n",
    "        self.endpoint = os.getenv(\"JANUARY_API_ENDPOINT\")\n",
    "        self.uuid = os.getenv(\"JANUARY_API_UUID\")\n",
    "        self.token = os.getenv(\"JANUARY_API_TOKEN\")\n",
    "\n",
    "        if not all([self.endpoint, self.uuid, self.token]):\n",
    "            raise ValueError(\n",
    "                \"January AI API credentials not found in environment variables.\"\n",
    "            )\n",
    "\n",
    "        self.headers = {\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"UUID\": self.uuid,\n",
    "            \"x-jan-e2e-tests-token\": self.token,\n",
    "        }\n",
    "        self.client = httpx.AsyncClient()\n",
    "\n",
    "    def _calculate_ingredient_macros(self, ingredient: dict) -> Dict[str, float]:\n",
    "        \"\"\"Re-implements legacy macro logic for a single ingredient.\"\"\"\n",
    "        (cal, fat, carbs, prot, mass, fiber) = (0, 0, 0, 0, 0, 0)\n",
    "        if \"servings\" in ingredient and ingredient[\"servings\"]:\n",
    "            s = ingredient[\"servings\"][0]\n",
    "            q, sel, scale = s[\"quantity\"], s[\"selected_quantity\"], s[\"scaling_factor\"]\n",
    "            w = s.get(\"weight_grams\", 0)\n",
    "            cal = ingredient[\"energy\"] * sel * scale / q\n",
    "            fat = ingredient[\"fat\"] * sel * scale / q\n",
    "            carbs = ingredient[\"carbs\"] * sel * scale / q\n",
    "            prot = ingredient[\"protein\"] * sel * scale / q\n",
    "            fib = ingredient.get(\"fiber\", 0) or 0\n",
    "            fiber = fib * sel * scale / q\n",
    "            mass = w * sel / q\n",
    "        return dict(\n",
    "            calories=cal, fat=fat, carbs=carbs, protein=prot, fiber=fiber, mass=mass\n",
    "        )\n",
    "\n",
    "    def _parse_server_response(self, js: Dict) -> FoodAnalysis:\n",
    "        \"\"\"Parses the raw JSON from the API into the standard FoodAnalysis schema.\"\"\"\n",
    "        ing_objs: List[Ingredient] = []\n",
    "        for ing in js.get(\"ingredients\", []):\n",
    "            m = self._calculate_ingredient_macros(ing)\n",
    "            ing_objs.append(\n",
    "                Ingredient(\n",
    "                    name=ing.get(\"name\", \"unknown ingredient\"),\n",
    "                    quantity=ing.get(\"servings\", [{}])[0].get(\"selected_quantity\", 0),\n",
    "                    unit=ing.get(\"servings\", [{}])[0].get(\"unit\", \"g\"),\n",
    "                    calories=m[\"calories\"],\n",
    "                    carbs=m[\"carbs\"],\n",
    "                    protein=m[\"protein\"],\n",
    "                    fat=m[\"fat\"],\n",
    "                )\n",
    "            )\n",
    "\n",
    "        tot = TotalMacros(\n",
    "            calories=sum(i.calories for i in ing_objs),\n",
    "            carbs=sum(i.carbs for i in ing_objs),\n",
    "            protein=sum(i.protein for i in ing_objs),\n",
    "            fat=sum(i.fat for i in ing_objs),\n",
    "        )\n",
    "\n",
    "        return FoodAnalysis(\n",
    "            meal_name=js.get(\"foodName\", \"unknown meal\"),\n",
    "            ingredients=ing_objs,\n",
    "            total_macros=tot,\n",
    "        )\n",
    "\n",
    "    async def analyse(self, img_path: Path) -> Tuple[Optional[dict], Optional[str]]:\n",
    "        \"\"\"\n",
    "        Analyzes an image using the January AI API.\n",
    "        Returns a structured dict, or None and an error message on failure.\n",
    "        \"\"\"\n",
    "        payload = {\"photoUrl\": img2b64(img_path)}\n",
    "        try:\n",
    "            r = await self.client.post(\n",
    "                self.endpoint, headers=self.headers, json=payload, timeout=30.0\n",
    "            )\n",
    "            r.raise_for_status()  # Raise an exception for bad status codes (4xx or 5xx)\n",
    "\n",
    "            js = r.json()\n",
    "            parsed_response = self._parse_server_response(js)\n",
    "\n",
    "            result = parsed_response.model_dump()\n",
    "            result[\"cost_usd\"] = self.COST_PER_IMAGE\n",
    "            result[\"prompt_variant\"] = \"default\"\n",
    "\n",
    "            return result, None\n",
    "\n",
    "        except httpx.HTTPStatusError as e:\n",
    "            error_msg = f\"API Error: {e.response.status_code} - {e.response.text}\"\n",
    "            print(f\"{error_msg} for {img_path.name}\")\n",
    "            return None, error_msg\n",
    "        except Exception as e:\n",
    "            error_msg = f\"Unexpected Error: {e}\"\n",
    "            print(f\"{error_msg} for {img_path.name}\")\n",
    "            return None, error_msg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df61fcca",
   "metadata": {},
   "source": [
    "### Metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedcffba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metrics:\n",
    "    \"\"\"Comprehensive metrics for food analysis evaluation.\"\"\"\n",
    "\n",
    "    _embedding_cache = {}\n",
    "\n",
    "    @staticmethod\n",
    "    def _normalize_ingredient_list(ingredients: Union[str, list, None]) -> List[dict]:\n",
    "        \"\"\"\n",
    "        IMPROVEMENT: Centralized cleanup function.\n",
    "        Safely parses ingredient data which might be a string representation of a list.\n",
    "        This avoids repeating the same try-except block in multiple metric functions.\n",
    "        \"\"\"\n",
    "        if not ingredients:\n",
    "            return []\n",
    "        if isinstance(ingredients, list):\n",
    "            return ingredients\n",
    "        if isinstance(ingredients, str):\n",
    "            try:\n",
    "                parsed = ast.literal_eval(ingredients)\n",
    "                return parsed if isinstance(parsed, list) else []\n",
    "            except (ValueError, SyntaxError):\n",
    "                return []\n",
    "        return []\n",
    "\n",
    "    @staticmethod\n",
    "    async def get_embedding(text, model=\"text-embedding-3-small\"):\n",
    "        \"\"\"Get OpenAI embedding with caching.\"\"\"\n",
    "        cache_key = hashlib.md5(f\"{text}_{model}\".encode()).hexdigest()\n",
    "        if cache_key in Metrics._embedding_cache:\n",
    "            return Metrics._embedding_cache[cache_key]\n",
    "        try:\n",
    "            client = openai.AsyncOpenAI()\n",
    "            response = await client.embeddings.create(model=model, input=text.strip())\n",
    "            embedding = response.data[0].embedding\n",
    "            Metrics._embedding_cache[cache_key] = embedding\n",
    "            return embedding\n",
    "        except Exception as e:\n",
    "            print(f\"Error getting embedding for '{text}': {e}\")\n",
    "            return [0.0] * 1536\n",
    "\n",
    "    @staticmethod\n",
    "    async def semantic_ingredient_match_embeddings(\n",
    "        gt_ingredients, pred_ingredients, threshold=0.75\n",
    "    ):\n",
    "        \"\"\"Semantic ingredient matching using OpenAI embeddings and cosine similarity.\"\"\"\n",
    "        gt_ingredients = Metrics._normalize_ingredient_list(gt_ingredients)\n",
    "        pred_ingredients = Metrics._normalize_ingredient_list(pred_ingredients)\n",
    "\n",
    "        def normalize_name(item):\n",
    "            return str(item.get(\"name\", \"\")).lower().strip()\n",
    "\n",
    "        gt_names = [normalize_name(x) for x in gt_ingredients if normalize_name(x)]\n",
    "        pred_names = [normalize_name(x) for x in pred_ingredients if normalize_name(x)]\n",
    "\n",
    "        if not gt_names and not pred_names:\n",
    "            return 1.0, []\n",
    "        if not gt_names or not pred_names:\n",
    "            return 0.0, []\n",
    "\n",
    "        gt_embeddings = await asyncio.gather(\n",
    "            *(Metrics.get_embedding(name) for name in gt_names)\n",
    "        )\n",
    "        pred_embeddings = await asyncio.gather(\n",
    "            *(Metrics.get_embedding(name) for name in pred_names)\n",
    "        )\n",
    "\n",
    "        gt_embeddings = [emb for emb in gt_embeddings if emb and len(emb) > 1]\n",
    "        pred_embeddings = [emb for emb in pred_embeddings if emb and len(emb) > 1]\n",
    "\n",
    "        if not gt_embeddings or not pred_embeddings:\n",
    "            return 0.0, []\n",
    "\n",
    "        similarity_matrix = cosine_similarity(gt_embeddings, pred_embeddings)\n",
    "        cost_matrix = 1 - similarity_matrix\n",
    "        row_indices, col_indices = linear_sum_assignment(cost_matrix)\n",
    "\n",
    "        matches = 0\n",
    "        match_details = []\n",
    "        for row_idx, col_idx in zip(row_indices, col_indices):\n",
    "            similarity = similarity_matrix[row_idx, col_idx]\n",
    "            if similarity >= threshold:\n",
    "                matches += 1\n",
    "                match_details.append(\n",
    "                    {\n",
    "                        \"gt_ingredient\": gt_names[row_idx],\n",
    "                        \"pred_ingredient\": pred_names[col_idx],\n",
    "                        \"similarity\": similarity,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "        recall = matches / len(gt_names)\n",
    "        return recall, match_details\n",
    "\n",
    "    @staticmethod\n",
    "    def semantic_ingredient_match(gt_ingredients, pred_ingredients, threshold=0.7):\n",
    "        \"\"\"Fallback method using string similarity.\"\"\"\n",
    "        gt_ingredients = Metrics._normalize_ingredient_list(gt_ingredients)\n",
    "        pred_ingredients = Metrics._normalize_ingredient_list(pred_ingredients)\n",
    "\n",
    "        def normalize_name(item):\n",
    "            name_str = item.get(\"name\", \"\") if isinstance(item, dict) else item\n",
    "            return str(name_str).lower().strip().replace(\"-\", \" \").replace(\"_\", \" \")\n",
    "\n",
    "        gt_names = [normalize_name(x) for x in gt_ingredients if normalize_name(x)]\n",
    "        pred_names = [normalize_name(x) for x in pred_ingredients if normalize_name(x)]\n",
    "\n",
    "        if not gt_names:\n",
    "            return 1.0 if not pred_names else 0.0\n",
    "\n",
    "        matches = 0\n",
    "        for gt_name in gt_names:\n",
    "            if pred_names:\n",
    "                best_match = max(\n",
    "                    SequenceMatcher(None, gt_name, pred_name).ratio()\n",
    "                    for pred_name in pred_names\n",
    "                )\n",
    "                if best_match >= threshold:\n",
    "                    matches += 1\n",
    "\n",
    "        return matches / len(gt_names)\n",
    "\n",
    "    @staticmethod\n",
    "    async def semantic_precision_score(\n",
    "        gt_ingredients, pred_ingredients, threshold=0.75\n",
    "    ):\n",
    "        \"\"\"\n",
    "        NEW: Calculates precision based on semantic matches from embeddings.\n",
    "        \"\"\"\n",
    "        gt_list = Metrics._normalize_ingredient_list(gt_ingredients)\n",
    "        pred_list = Metrics._normalize_ingredient_list(pred_ingredients)\n",
    "\n",
    "        def normalize_name(item):\n",
    "            return str(item.get(\"name\", \"\")).lower().strip()\n",
    "\n",
    "        gt_names = [normalize_name(x) for x in gt_list if normalize_name(x)]\n",
    "        pred_names = [normalize_name(x) for x in pred_list if normalize_name(x)]\n",
    "\n",
    "        if not pred_names:\n",
    "            return 1.0\n",
    "        if not gt_names:\n",
    "            return 0.0\n",
    "\n",
    "        _, match_details = await Metrics.semantic_ingredient_match_embeddings(\n",
    "            gt_list, pred_list, threshold\n",
    "        )\n",
    "\n",
    "        matches = len(match_details)\n",
    "        precision = matches / len(pred_names)\n",
    "        return precision\n",
    "\n",
    "    @staticmethod\n",
    "    async def semantic_f1_score(gt_ingredients, pred_ingredients, threshold=0.75):\n",
    "        gt_list = Metrics._normalize_ingredient_list(gt_ingredients)\n",
    "        pred_list = Metrics._normalize_ingredient_list(pred_ingredients)\n",
    "\n",
    "        def normalize_name(item):\n",
    "            return str(item.get(\"name\", \"\")).lower().strip()\n",
    "\n",
    "        gt_names = [normalize_name(x) for x in gt_list if normalize_name(x)]\n",
    "        pred_names = [normalize_name(x) for x in pred_list if normalize_name(x)]\n",
    "\n",
    "        if not gt_names and not pred_names:\n",
    "            return 1.0\n",
    "        if not gt_names or not pred_names:\n",
    "            return 0.0\n",
    "\n",
    "        _, match_details = await Metrics.semantic_ingredient_match_embeddings(\n",
    "            gt_list, pred_list, threshold\n",
    "        )\n",
    "\n",
    "        matches = len(match_details)\n",
    "\n",
    "        if not pred_names or not gt_names:\n",
    "            return 0.0\n",
    "\n",
    "        precision = matches / len(pred_names)\n",
    "        recall = matches / len(gt_names)\n",
    "\n",
    "        if precision + recall == 0:\n",
    "            return 0.0\n",
    "\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "        return f1_score\n",
    "\n",
    "    @staticmethod\n",
    "    async def meal_name_similarity(gt_name: str, pred_name: str) -> float:\n",
    "        \"\"\"Calculates cosine similarity between the embeddings of two meal names.\"\"\"\n",
    "        gt_name = str(gt_name or \"\").strip()\n",
    "        pred_name = str(pred_name or \"\").strip()\n",
    "\n",
    "        if not pred_name:\n",
    "            return 0.0\n",
    "\n",
    "        gt_embedding_list, pred_embedding_list = await asyncio.gather(\n",
    "            Metrics.get_embedding(gt_name), Metrics.get_embedding(pred_name)\n",
    "        )\n",
    "\n",
    "        gt_embedding = np.array(gt_embedding_list).reshape(1, -1)\n",
    "        pred_embedding = np.array(pred_embedding_list).reshape(1, -1)\n",
    "\n",
    "        return cosine_similarity(gt_embedding, pred_embedding)[0][0]\n",
    "\n",
    "    @staticmethod\n",
    "    def macro_wMAPE(gt_mac: dict, pred_mac: dict):\n",
    "        \"\"\"Calculates Weighted Mean Absolute Percentage Error over the four main macros.\"\"\"\n",
    "        keys = [\"calories\", \"carbs\", \"protein\", \"fat\"]\n",
    "\n",
    "        absolute_errors = sum(abs(gt_mac.get(k, 0) - pred_mac.get(k, 0)) for k in keys)\n",
    "        sum_of_actuals = sum(abs(gt_mac.get(k, 0)) for k in keys)\n",
    "\n",
    "        if sum_of_actuals == 0:\n",
    "            return 0.0 if absolute_errors == 0 else 100.0\n",
    "\n",
    "        return (absolute_errors / sum_of_actuals) * 100\n",
    "\n",
    "    @staticmethod\n",
    "    def macro_percentage_error(gt_mac, pred_mac):\n",
    "        \"\"\"Calculate percentage error for each macro.\"\"\"\n",
    "        keys = [\"calories\", \"carbs\", \"protein\", \"fat\"]\n",
    "        errors = {}\n",
    "        for key in keys:\n",
    "            gt_val = gt_mac.get(key, 0)\n",
    "            pred_val = pred_mac.get(key, 0)\n",
    "            if gt_val > 0:\n",
    "                errors[key] = abs(gt_val - pred_val) / gt_val * 100\n",
    "            else:\n",
    "                errors[key] = 0 if pred_val == 0 else 100\n",
    "        return errors\n",
    "\n",
    "    @staticmethod\n",
    "    def ingredient_count_accuracy(gt_ingredients, pred_ingredients):\n",
    "        \"\"\"How well does the model predict the number of ingredients?\"\"\"\n",
    "        gt_count = len(Metrics._normalize_ingredient_list(gt_ingredients))\n",
    "        pred_count = len(Metrics._normalize_ingredient_list(pred_ingredients))\n",
    "        if gt_count == 0 and pred_count == 0:\n",
    "            return 1.0\n",
    "        if gt_count == 0:\n",
    "            return 0.0\n",
    "        return 1 - abs(gt_count - pred_count) / gt_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7885e9f",
   "metadata": {},
   "source": [
    "## Evaluation Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f880e9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def _process_sample(\n",
    "    idx: int,\n",
    "    ds: FoodScanDataset,\n",
    "    llm: Union[LiteModel, JanuaryAIModel],\n",
    "    model_name: str,\n",
    "    use_embeddings: bool = True,\n",
    ") -> dict:\n",
    "    start_time = time.time()\n",
    "    sample = ds[idx]\n",
    "    pred, error_msg = await llm.analyse(sample[\"image_path\"])\n",
    "    end_time = time.time()\n",
    "    gt = sample[\"gt\"]\n",
    "\n",
    "    item = {\n",
    "        \"image_id\": sample[\"image_id\"],\n",
    "        \"model\": model_name,\n",
    "        \"response_time_seconds\": round(end_time - start_time, 2),\n",
    "    }\n",
    "\n",
    "    if pred is None:\n",
    "        item.update(\n",
    "            {\n",
    "                \"meal_name_similarity\": 0.0,\n",
    "                \"semantic_precision_ing\": 0.0,\n",
    "                \"semantic_match\": 0.0,\n",
    "                \"semantic_f1_ing\": 0.0,\n",
    "                \"semantic_match_embeddings\": 0.0,\n",
    "                \"ingredient_count_acc\": 0.0,\n",
    "                \"wmape_mac\": None,\n",
    "                \"error\": error_msg or \"failed\",\n",
    "                \"cost_usd\": 0.0,\n",
    "                \"calories_pct_error\": None,\n",
    "                \"carbs_pct_error\": None,\n",
    "                \"protein_pct_error\": None,\n",
    "                \"fat_pct_error\": None,\n",
    "                \"match_details\": None,\n",
    "            }\n",
    "        )\n",
    "    else:\n",
    "        gt_ingredients = gt[\"ingredients\"]\n",
    "        pred_ingredients = pred.get(\"ingredients\", [])\n",
    "        gt_macros = gt[\"macros\"]\n",
    "        pred_macros = pred.get(\"total_macros\", {})\n",
    "\n",
    "        meal_name_sim = await Metrics.meal_name_similarity(\n",
    "            gt[\"meal_name\"], pred.get(\"meal_name\", \"\")\n",
    "        )\n",
    "\n",
    "        semantic_match_embeddings, match_details = 0.0, None\n",
    "        semantic_f1 = 0.0\n",
    "        semantic_precision = 0.0\n",
    "        if use_embeddings:\n",
    "            try:\n",
    "                (\n",
    "                    semantic_match_embeddings,\n",
    "                    match_details,\n",
    "                ) = await Metrics.semantic_ingredient_match_embeddings(\n",
    "                    gt_ingredients, pred_ingredients\n",
    "                )\n",
    "                semantic_f1 = await Metrics.semantic_f1_score(\n",
    "                    gt_ingredients, pred_ingredients\n",
    "                )\n",
    "                semantic_precision = await Metrics.semantic_precision_score(\n",
    "                    gt_ingredients, pred_ingredients\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(\n",
    "                    f\"Error in embedding similarity for image {sample['image_id']}: {e}\"\n",
    "                )\n",
    "                semantic_match_embeddings = Metrics.semantic_ingredient_match(\n",
    "                    gt_ingredients, pred_ingredients\n",
    "                )\n",
    "                semantic_f1 = 0.0\n",
    "                semantic_precision = 0.0\n",
    "\n",
    "        pct_errors = Metrics.macro_percentage_error(gt_macros, pred_macros)\n",
    "\n",
    "        item.update(\n",
    "            {\n",
    "                \"meal_name\": pred.get(\"meal_name\", \"\"),\n",
    "                \"gt_meal_name\": gt[\"meal_name\"],\n",
    "                \"meal_name_similarity\": meal_name_sim,\n",
    "                \"semantic_precision_ing\": semantic_precision,\n",
    "                \"semantic_match\": Metrics.semantic_ingredient_match(\n",
    "                    gt_ingredients, pred_ingredients\n",
    "                ),\n",
    "                \"semantic_f1_ing\": semantic_f1,\n",
    "                \"semantic_match_embeddings\": semantic_match_embeddings,\n",
    "                \"ingredient_count_acc\": Metrics.ingredient_count_accuracy(\n",
    "                    gt_ingredients, pred_ingredients\n",
    "                ),\n",
    "                \"wmape_mac\": Metrics.macro_wMAPE(gt_macros, pred_macros),\n",
    "                \"error\": None,\n",
    "                \"cost_usd\": pred.get(\"cost_usd\", 0.0),\n",
    "                \"calories_pct_error\": pct_errors.get(\"calories\"),\n",
    "                \"carbs_pct_error\": pct_errors.get(\"carbs\"),\n",
    "                \"protein_pct_error\": pct_errors.get(\"protein\"),\n",
    "                \"fat_pct_error\": pct_errors.get(\"fat\"),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return item\n",
    "\n",
    "\n",
    "async def run_evaluation(\n",
    "    models: Union[str, List[str]],\n",
    "    cache_dir: Path,\n",
    "    max_items: Optional[int] = None,\n",
    "    max_concurrent: int = 5,\n",
    "    use_embeddings: bool = True,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Run evaluation with multiple models, including custom ones.\"\"\"\n",
    "    models_to_run = [models] if isinstance(models, str) else models\n",
    "    all_prompt_variants = list(LiteModel.PROMPT_VARIANTS.keys())\n",
    "\n",
    "    ds = FoodScanDataset(cache_dir)\n",
    "    n = min(max_items, len(ds)) if max_items else len(ds)\n",
    "    tasks_to_run = []\n",
    "    for model_name in models_to_run:\n",
    "        if model_name == \"january/food-vision-v1\":\n",
    "            tasks_to_run.append({\"model_name\": model_name, \"prompt_variant\": \"default\"})\n",
    "        else:\n",
    "            for variant in all_prompt_variants:\n",
    "                tasks_to_run.append(\n",
    "                    {\"model_name\": model_name, \"prompt_variant\": variant}\n",
    "                )\n",
    "\n",
    "    sem = asyncio.Semaphore(max_concurrent)\n",
    "\n",
    "    january_model = None\n",
    "    if any(t[\"model_name\"] == \"january/food-vision-v1\" for t in tasks_to_run):\n",
    "        january_model = JanuaryAIModel()\n",
    "\n",
    "    async def _worker(task_info: dict, idx: int):\n",
    "        async with sem:\n",
    "            model_name = task_info[\"model_name\"]\n",
    "            prompt_variant = task_info[\"prompt_variant\"]\n",
    "\n",
    "            if model_name == \"january/food-vision-v1\":\n",
    "                llm = january_model\n",
    "                model_name_for_results = model_name\n",
    "            else:\n",
    "                llm = LiteModel(model_name, prompt_variant=prompt_variant)\n",
    "                model_name_for_results = f\"{model_name}_{prompt_variant}\"\n",
    "\n",
    "            return await _process_sample(\n",
    "                idx, ds, llm, model_name_for_results, use_embeddings\n",
    "            )\n",
    "\n",
    "    all_jobs = [(i, task) for i in range(n) for task in tasks_to_run]\n",
    "\n",
    "    pbar = tqdm(total=len(all_jobs), desc=\"Processing images\", dynamic_ncols=True)\n",
    "\n",
    "    async def _job_runner(job):\n",
    "        result = await _worker(job[1], job[0])\n",
    "        pbar.update(1)\n",
    "        return result\n",
    "\n",
    "    results = await asyncio.gather(*[_job_runner(job) for job in all_jobs])\n",
    "    pbar.close()\n",
    "\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2530ecd7",
   "metadata": {},
   "source": [
    "## Analysis and Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa733757",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BenchmarkAnalyzer:\n",
    "    \"\"\"Comprehensive analysis and visualization of benchmark results.\"\"\"\n",
    "\n",
    "    def __init__(self, results_df):\n",
    "        self.df = results_df.copy()\n",
    "\n",
    "        def get_base_model(model_name):\n",
    "            if model_name == \"january/food-vision-v1\":\n",
    "                return model_name\n",
    "\n",
    "            for variant in LiteModel.PROMPT_VARIANTS:\n",
    "                if model_name.endswith(f\"_{variant}\"):\n",
    "                    return model_name.rsplit(f\"_{variant}\", 1)[0]\n",
    "\n",
    "            return model_name\n",
    "\n",
    "        self.df[\"base_model\"] = self.df[\"model\"].apply(get_base_model)\n",
    "        self.df[\"pretty_model\"] = self.df[\"model\"].apply(pretty_label)\n",
    "\n",
    "        if not self.df.empty:\n",
    "            self.successful_df = self.df[self.df[\"error\"].isna()].copy()\n",
    "\n",
    "            if not self.successful_df.empty:\n",
    "                self.successful_df = self._add_overall_score(self.successful_df)\n",
    "\n",
    "                best_indices = self.successful_df.groupby([\"image_id\", \"base_model\"])[\n",
    "                    \"overall_score\"\n",
    "                ].idxmax()\n",
    "                self.best_of_df = self.successful_df.loc[best_indices].copy()\n",
    "                self.best_of_df[\"model\"] = self.best_of_df[\"base_model\"].apply(\n",
    "                    lambda x: f\"{get_display_name(x)} (Best)\"\n",
    "                    if x != \"january/food-vision-v1\"\n",
    "                    else get_display_name(x)\n",
    "                )\n",
    "\n",
    "                numeric_cols = [\n",
    "                    \"meal_name_similarity\",\n",
    "                    \"semantic_match_embeddings\",\n",
    "                    \"semantic_precision_ing\",\n",
    "                    \"semantic_f1_ing\",\n",
    "                    \"ingredient_count_acc\",\n",
    "                    \"wmape_mac\",\n",
    "                    \"cost_usd\",\n",
    "                    \"response_time_seconds\",\n",
    "                    \"calories_pct_error\",\n",
    "                    \"carbs_pct_error\",\n",
    "                    \"protein_pct_error\",\n",
    "                    \"fat_pct_error\",\n",
    "                ]\n",
    "                self.average_of_df = (\n",
    "                    self.successful_df.groupby([\"image_id\", \"base_model\"])[numeric_cols]\n",
    "                    .mean()\n",
    "                    .reset_index()\n",
    "                )\n",
    "                self.average_of_df[\"model\"] = self.average_of_df[\"base_model\"].apply(\n",
    "                    lambda x: f\"{get_display_name(x)} (Avg)\"\n",
    "                    if x != \"january/food-vision-v1\"\n",
    "                    else get_display_name(x)\n",
    "                )\n",
    "\n",
    "                self.average_of_df = self._add_overall_score(self.average_of_df)\n",
    "\n",
    "                self.plot_df = pd.concat(\n",
    "                    [self.best_of_df, self.average_of_df], ignore_index=True\n",
    "                ).drop_duplicates(subset=[\"model\", \"image_id\"], keep=\"first\")\n",
    "\n",
    "            else:\n",
    "                self.best_of_df = pd.DataFrame()\n",
    "                self.average_of_df = pd.DataFrame()\n",
    "                self.plot_df = pd.DataFrame()\n",
    "\n",
    "        else:\n",
    "            self.successful_df = pd.DataFrame()\n",
    "            self.best_of_df = pd.DataFrame()\n",
    "            self.average_of_df = pd.DataFrame()\n",
    "            self.plot_df = pd.DataFrame()\n",
    "\n",
    "    def _add_overall_score(self, df):\n",
    "        \"\"\"\n",
    "        Calculates a overall score using a weighted geometric mean.\n",
    "        This method is robust to outliers and uses a \"knock-out\" criterion,\n",
    "        where a score of 0 in any key metric results in an overall score of 0.\n",
    "\n",
    "        IMPROVEMENT: This version uses absolute normalization for cost and speed,\n",
    "        making the score comparable across different benchmark runs.\n",
    "        \"\"\"\n",
    "        if df.empty:\n",
    "            return df\n",
    "\n",
    "        COST_CEILING = 1\n",
    "        TIME_CEILING = 60\n",
    "\n",
    "        weights = {\n",
    "            \"name_similarity\": 0.15,\n",
    "            \"ing_accuracy\": 0.40,\n",
    "            \"macro_accuracy\": 0.25,\n",
    "            \"cost\": 0.10,\n",
    "            \"speed\": 0.10,\n",
    "        }\n",
    "        assert np.isclose(sum(weights.values()), 1.0), \"Weights must sum to 1.0\"\n",
    "\n",
    "        norm_name_sim = df[\"meal_name_similarity\"].clip(0, 1)\n",
    "        norm_acc_ing = df[\"semantic_f1_ing\"].clip(0, 1)\n",
    "        norm_acc_macro = (1 - (df[\"wmape_mac\"] / 100)).clip(0, 1)\n",
    "\n",
    "        clipped_cost = df[\"cost_usd\"].clip(upper=COST_CEILING)\n",
    "        norm_cost = 1 - (clipped_cost / COST_CEILING)\n",
    "\n",
    "        clipped_time = df[\"response_time_seconds\"].clip(upper=TIME_CEILING)\n",
    "        norm_speed = 1 - (clipped_time / TIME_CEILING)\n",
    "\n",
    "        df[\"overall_score\"] = 100 * (\n",
    "            (norm_name_sim ** weights[\"name_similarity\"])\n",
    "            * (norm_acc_ing ** weights[\"ing_accuracy\"])\n",
    "            * (norm_acc_macro ** weights[\"macro_accuracy\"])\n",
    "            * (norm_cost ** weights[\"cost\"])\n",
    "            * (norm_speed ** weights[\"speed\"])\n",
    "        )\n",
    "\n",
    "        df[\"norm_name_similarity\"] = norm_name_sim\n",
    "        df[\"norm_ing_accuracy\"] = norm_acc_ing\n",
    "        df[\"norm_macro_accuracy\"] = norm_acc_macro\n",
    "        df[\"norm_cost\"] = norm_cost\n",
    "        df[\"norm_speed\"] = norm_speed\n",
    "\n",
    "        return df\n",
    "\n",
    "    def summary_statistics(self):\n",
    "        \"\"\"Generate comprehensive summary statistics for individual variants and aggregated models.\"\"\"\n",
    "        print(\"=== BENCHMARK SUMMARY (PER VARIANT) ===\\n\")\n",
    "        if self.df.empty:\n",
    "            print(\"No results to analyze.\")\n",
    "            return\n",
    "\n",
    "        for model in sorted(self.df[\"model\"].unique()):\n",
    "            display_name = pretty_label(model)\n",
    "            model_df = self.df[self.df[\"model\"] == model]\n",
    "            successful = model_df[model_df[\"error\"].isna()]\n",
    "            success_rate = (\n",
    "                (len(successful) / len(model_df)) * 100 if len(model_df) > 0 else 0\n",
    "            )\n",
    "\n",
    "            print(f\"--- {display_name} ---\")\n",
    "            print(\n",
    "                f\"  Success Rate: {success_rate:.1f}% ({len(successful)}/{len(model_df)})\"\n",
    "            )\n",
    "\n",
    "            if not successful.empty:\n",
    "                print(\n",
    "                    f\"  Avg Semantic Match (Embeddings): {successful['semantic_match_embeddings'].mean():.3f}\"\n",
    "                )\n",
    "                print(\n",
    "                    f\"  Avg Semantic Precision (Ingredients): {successful['semantic_precision_ing'].mean():.3f}\"\n",
    "                )\n",
    "                print(f\"  Avg wMAPE (Macros): {successful['wmape_mac'].mean():.1f}%\")\n",
    "                print(f\"  Avg Cost per Image: ${successful['cost_usd'].mean():.4f}\")\n",
    "                print(\n",
    "                    f\"  Avg Response Time: {successful['response_time_seconds'].mean():.1f}s\\n\"\n",
    "                )\n",
    "\n",
    "        print(\"\\n=== AGGREGATED SUMMARY (BEST OF N PROMPTS) ===\\n\")\n",
    "        if self.best_of_df.empty:\n",
    "            print(\"No successful results to analyze for aggregation.\")\n",
    "        else:\n",
    "            agg_best_df = (\n",
    "                self.best_of_df.groupby(\"model\")\n",
    "                .agg(\n",
    "                    semantic_match_embeddings=(\"semantic_match_embeddings\", \"mean\"),\n",
    "                    semantic_precision_ing=(\"semantic_precision_ing\", \"mean\"),\n",
    "                    wmape_mac=(\"wmape_mac\", \"mean\"),\n",
    "                    cost_usd=(\"cost_usd\", \"mean\"),\n",
    "                    response_time_seconds=(\"response_time_seconds\", \"mean\"),\n",
    "                    overall_score=(\"overall_score\", \"mean\"),\n",
    "                    sample_count=(\"image_id\", \"count\"),\n",
    "                )\n",
    "                .reset_index()\n",
    "            )\n",
    "\n",
    "            for _, row in agg_best_df.iterrows():\n",
    "                print(f\"--- {row['model']} (from {row['sample_count']} samples) ---\")\n",
    "                print(\n",
    "                    f\"  Avg Semantic Match (Embeddings): {row['semantic_match_embeddings']:.3f}\"\n",
    "                )\n",
    "                print(\n",
    "                    f\"  Avg Semantic Precision (Ingredients): {row['semantic_precision_ing']:.3f}\"\n",
    "                )\n",
    "                print(f\"  Avg wMAPE (Macros): {row['wmape_mac']:.1f}%\")\n",
    "                print(f\"  Avg Cost per Image: ${row['cost_usd']:.4f}\")\n",
    "                print(f\"  Avg Response Time: {row['response_time_seconds']:.1f}s\\n\")\n",
    "                print(f\"  overall Score: {row['overall_score']:.2f} / 100\\n\")\n",
    "\n",
    "        print(\"\\n=== AGGREGATED SUMMARY (AVERAGE) ===\\n\")\n",
    "        if self.average_of_df.empty:\n",
    "            print(\"No successful results to analyze for aggregation.\")\n",
    "            self.analyze_errors()\n",
    "            return\n",
    "\n",
    "        agg_df = (\n",
    "            self.average_of_df.groupby(\"model\")\n",
    "            .agg(\n",
    "                semantic_match_embeddings=(\"semantic_match_embeddings\", \"mean\"),\n",
    "                semantic_precision_ing=(\"semantic_precision_ing\", \"mean\"),\n",
    "                wmape_mac=(\"wmape_mac\", \"mean\"),\n",
    "                cost_usd=(\"cost_usd\", \"mean\"),\n",
    "                response_time_seconds=(\"response_time_seconds\", \"mean\"),\n",
    "                overall_score=(\"overall_score\", \"mean\"),\n",
    "                sample_count=(\"image_id\", \"count\"),\n",
    "            )\n",
    "            .reset_index()\n",
    "        )\n",
    "\n",
    "        for _, row in agg_df.iterrows():\n",
    "            print(f\"--- {row['model']} (from {row['sample_count']} samples) ---\")\n",
    "            print(\n",
    "                f\"  Avg Semantic Match (Embeddings): {row['semantic_match_embeddings']:.3f}\"\n",
    "            )\n",
    "            print(\n",
    "                f\"  Avg Semantic Precision (Ingredients): {row['semantic_precision_ing']:.3f}\"\n",
    "            )\n",
    "            print(f\"  Avg wMAPE (Macros): {row['wmape_mac']:.1f}%\")\n",
    "            print(f\"  Avg Cost per Image: ${row['cost_usd']:.4f}\")\n",
    "            print(f\"  Avg Response Time: {row['response_time_seconds']:.1f}s\\n\")\n",
    "            print(f\"  overall Score: {row['overall_score']:.2f} / 100\\n\")\n",
    "\n",
    "        self.analyze_errors()\n",
    "\n",
    "    def analyze_errors(self):\n",
    "        \"\"\"Analyze and summarize the specific errors encountered.\"\"\"\n",
    "        print(\"--- ERROR ANALYSIS ---\")\n",
    "        error_df = self.df[self.df[\"error\"].notna()]\n",
    "        if error_df.empty:\n",
    "            print(\"No errors encountered. All API calls were successful.\\n\")\n",
    "            return\n",
    "\n",
    "        print(\"Error counts by model:\")\n",
    "        error_summary = error_df.groupby(\"model\")[\"error\"].count().sort_index()\n",
    "        print(error_summary.to_string())\n",
    "\n",
    "        print(\"\\nMost common error messages:\")\n",
    "        common_errors = (\n",
    "            error_df[\"error\"].str.split(\":\").str[0].value_counts().nlargest(5)\n",
    "        )\n",
    "        print(common_errors.to_string())\n",
    "        print()\n",
    "\n",
    "    def create_performance_dashboard(self):\n",
    "        \"\"\"Create a comprehensive performance comparison dashboard with improved styling.\"\"\"\n",
    "        if self.plot_df.empty:\n",
    "            print(\"No successful predictions to plot.\")\n",
    "            return\n",
    "\n",
    "        models = sorted(self.plot_df[\"model\"].unique())\n",
    "\n",
    "        short_labels = {m: m for m in models}\n",
    "        idx_map = {m: i + 1 for i, m in enumerate(short_labels)}\n",
    "\n",
    "        colors = px.colors.qualitative.Plotly\n",
    "\n",
    "        fig = make_subplots(\n",
    "            rows=2,\n",
    "            cols=3,\n",
    "            subplot_titles=(\n",
    "                \"Meal Name Similarity\",\n",
    "                \"Macro Nutritional wMAPE (%)\",\n",
    "                \"Response Time Distribution\",\n",
    "                \"Recall (Ingredients)\",\n",
    "                \"Precision (Ingredients)\",\n",
    "                \"F1 Score (Ingredients)\",\n",
    "            ),\n",
    "            vertical_spacing=0.15,\n",
    "            horizontal_spacing=0.08,\n",
    "        )\n",
    "\n",
    "        for i, model in enumerate(models):\n",
    "            m_idx = str(idx_map[model])\n",
    "            color = colors[i % len(colors)]\n",
    "            d_ok = self.plot_df[self.plot_df[\"model\"] == model]\n",
    "            if d_ok.empty:\n",
    "                continue\n",
    "\n",
    "            box_style = dict(\n",
    "                marker_color=color,\n",
    "                marker_line_color=\"rgba(0,0,0,0.3)\",\n",
    "                marker_line_width=1,\n",
    "                line_color=color,\n",
    "                line_width=2,\n",
    "            )\n",
    "            fig.add_trace(\n",
    "                go.Box(\n",
    "                    y=d_ok[\"meal_name_similarity\"],\n",
    "                    name=m_idx,\n",
    "                    legendgroup=m_idx,\n",
    "                    showlegend=False,\n",
    "                    **box_style,\n",
    "                ),\n",
    "                row=1,\n",
    "                col=1,\n",
    "            )\n",
    "            fig.add_trace(\n",
    "                go.Box(\n",
    "                    y=d_ok[\"wmape_mac\"],\n",
    "                    name=m_idx,\n",
    "                    legendgroup=m_idx,\n",
    "                    showlegend=False,\n",
    "                    **box_style,\n",
    "                ),\n",
    "                row=1,\n",
    "                col=2,\n",
    "            )\n",
    "            fig.add_trace(\n",
    "                go.Box(\n",
    "                    y=d_ok[\"response_time_seconds\"],\n",
    "                    name=m_idx,\n",
    "                    legendgroup=m_idx,\n",
    "                    showlegend=False,\n",
    "                    **box_style,\n",
    "                ),\n",
    "                row=1,\n",
    "                col=3,\n",
    "            )\n",
    "            fig.add_trace(\n",
    "                go.Box(\n",
    "                    y=d_ok[\"semantic_match_embeddings\"],\n",
    "                    name=m_idx,\n",
    "                    legendgroup=m_idx,\n",
    "                    showlegend=False,\n",
    "                    **box_style,\n",
    "                ),\n",
    "                row=2,\n",
    "                col=1,\n",
    "            )\n",
    "            fig.add_trace(\n",
    "                go.Box(\n",
    "                    y=d_ok[\"semantic_precision_ing\"],\n",
    "                    name=m_idx,\n",
    "                    legendgroup=m_idx,\n",
    "                    showlegend=False,\n",
    "                    **box_style,\n",
    "                ),\n",
    "                row=2,\n",
    "                col=2,\n",
    "            )\n",
    "            fig.add_trace(\n",
    "                go.Box(\n",
    "                    y=d_ok[\"semantic_f1_ing\"],\n",
    "                    name=m_idx,\n",
    "                    legendgroup=m_idx,\n",
    "                    showlegend=False,\n",
    "                    **box_style,\n",
    "                ),\n",
    "                row=2,\n",
    "                col=3,\n",
    "            )\n",
    "\n",
    "        for i, model in enumerate(models):\n",
    "            m_idx = str(idx_map[model])\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=[None],\n",
    "                    y=[None],\n",
    "                    mode=\"markers\",\n",
    "                    marker=dict(\n",
    "                        size=12,\n",
    "                        color=colors[i % len(colors)],\n",
    "                        symbol=\"circle\",\n",
    "                        line=dict(width=2, color=\"rgba(0,0,0,0.3)\"),\n",
    "                    ),\n",
    "                    legendgroup=m_idx,\n",
    "                    showlegend=True,\n",
    "                    name=f\"{m_idx}: {short_labels[model]}\",\n",
    "                )\n",
    "            )\n",
    "\n",
    "        axis_style = dict(\n",
    "            showgrid=True,\n",
    "            gridcolor=\"rgba(128,128,128,0.2)\",\n",
    "            gridwidth=1,\n",
    "            zeroline=True,\n",
    "            zerolinecolor=\"rgba(128,128,128,0.4)\",\n",
    "            zerolinewidth=1,\n",
    "            tickfont=dict(size=11, color=\"#2f2f2f\"),\n",
    "        )\n",
    "        for row in [1, 2]:\n",
    "            for col in [1, 2, 3]:\n",
    "                fig.update_xaxes(**axis_style, row=row, col=col)\n",
    "                fig.update_yaxes(**axis_style, row=row, col=col)\n",
    "        fig.update_yaxes(title_text=\"Cosine Similarity\", row=1, col=1)\n",
    "        fig.update_yaxes(title_text=\"Weighted MAPE (%)\", row=1, col=2)\n",
    "        fig.update_yaxes(title_text=\"Response Time (sec)\", row=1, col=3)\n",
    "        fig.update_yaxes(title_text=\"Recall\", row=2, col=1)\n",
    "        fig.update_yaxes(title_text=\"Precision\", row=2, col=2)\n",
    "        fig.update_yaxes(title_text=\"F1 Score\", row=2, col=3)\n",
    "        fig.update_layout(\n",
    "            height=800,\n",
    "            width=1400,\n",
    "            title=dict(\n",
    "                text=\"<b>Model Performance Dashboard</b>\",\n",
    "                x=0.5,\n",
    "                xanchor=\"center\",\n",
    "                font=dict(size=24, color=\"#1f1f1f\", family=\"Arial Black\"),\n",
    "            ),\n",
    "            showlegend=True,\n",
    "            legend=dict(\n",
    "                title=\"<b>Models</b>\",\n",
    "                title_font=dict(size=14, color=\"#1f1f1f\"),\n",
    "                font=dict(size=11),\n",
    "                bgcolor=\"rgba(255,255,255,0.8)\",\n",
    "                bordercolor=\"rgba(128,128,128,0.5)\",\n",
    "                borderwidth=1,\n",
    "                x=1.02,\n",
    "                y=1,\n",
    "                xanchor=\"left\",\n",
    "                yanchor=\"top\",\n",
    "            ),\n",
    "            plot_bgcolor=\"rgba(248,249,250,0.8)\",\n",
    "            paper_bgcolor=\"white\",\n",
    "            font=dict(family=\"Arial, sans-serif\", size=11, color=\"#2f2f2f\"),\n",
    "            margin=dict(l=80, r=200, t=120, b=80),\n",
    "            hovermode=\"closest\",\n",
    "        )\n",
    "        config = dict(\n",
    "            displayModeBar=True,\n",
    "            displaylogo=False,\n",
    "            modeBarButtonsToRemove=[\"pan2d\", \"lasso2d\"],\n",
    "            toImageButtonOptions=dict(\n",
    "                format=\"png\",\n",
    "                filename=\"model_performance_dashboard\",\n",
    "                height=800,\n",
    "                width=1400,\n",
    "                scale=2,\n",
    "            ),\n",
    "        )\n",
    "        fig.show(config=config)\n",
    "        fig.write_html(\"performance_dashboard.html\", config=config)\n",
    "\n",
    "    def export_detailed_report(self, filename=\"benchmark_report.csv\"):\n",
    "        \"\"\"Export a detailed report with raw data and summary to CSV files.\"\"\"\n",
    "        print(f\"Exporting detailed report to {filename}...\")\n",
    "        self.df.to_csv(filename, index=False)\n",
    "\n",
    "        if not self.plot_df.empty:\n",
    "            summary = (\n",
    "                self.plot_df.groupby(\"model\")\n",
    "                .agg(\n",
    "                    semantic_match_embeddings_mean=(\n",
    "                        \"semantic_match_embeddings\",\n",
    "                        \"mean\",\n",
    "                    ),\n",
    "                    semantic_precision_ing_mean=(\"semantic_precision_ing\", \"mean\"),\n",
    "                    wmape_mac_mean=(\"wmape_mac\", \"mean\"),\n",
    "                    cost_usd_total=(\"cost_usd\", \"sum\"),\n",
    "                    response_time_seconds_mean=(\"response_time_seconds\", \"mean\"),\n",
    "                )\n",
    "                .reset_index()\n",
    "            )\n",
    "            summary_filename = filename.replace(\".csv\", \"_summary.csv\")\n",
    "            summary.to_csv(summary_filename, index=False)\n",
    "        print(\"Export complete.\")\n",
    "\n",
    "    def create_win_loss_analysis(self, baseline_model_name: Optional[str] = None):\n",
    "        if self.average_of_df.empty or self.best_of_df.empty:\n",
    "            print(\"No successful results to analyze.\")\n",
    "            return\n",
    "\n",
    "        base_models = sorted(self.average_of_df[\"base_model\"].unique())\n",
    "\n",
    "        if len(base_models) < 2:\n",
    "            print(\"Need at least two models to perform a win-loss comparison.\")\n",
    "            return\n",
    "\n",
    "        if baseline_model_name:\n",
    "            if baseline_model_name not in base_models:\n",
    "                print(\n",
    "                    f\"Error: Baseline model '{baseline_model_name}' not found in results.\"\n",
    "                )\n",
    "                print(f\"Available models are: {base_models}\")\n",
    "                return\n",
    "            baseline_model = baseline_model_name\n",
    "        else:\n",
    "            baseline_model = (\n",
    "                \"january/food-vision-v1\"\n",
    "                if \"january/food-vision-v1\" in base_models\n",
    "                else base_models[0]\n",
    "            )\n",
    "            print(\n",
    "                f\"INFO: No baseline model specified. Using average performance of '{get_display_name(baseline_model)}' as the default.\"\n",
    "            )\n",
    "\n",
    "        baseline_data = self.average_of_df[\n",
    "            self.average_of_df[\"base_model\"] == baseline_model\n",
    "        ].copy()\n",
    "\n",
    "        competitor_dfs = {\n",
    "            \"Avg\": self.average_of_df[\n",
    "                self.average_of_df[\"base_model\"] != baseline_model\n",
    "            ],\n",
    "            \"Best\": self.best_of_df[self.best_of_df[\"base_model\"] != baseline_model],\n",
    "        }\n",
    "\n",
    "        macro_cols = [\n",
    "            \"calories_pct_error\",\n",
    "            \"carbs_pct_error\",\n",
    "            \"protein_pct_error\",\n",
    "            \"fat_pct_error\",\n",
    "        ]\n",
    "        win_loss_data = {}\n",
    "\n",
    "        for perf_type, df in competitor_dfs.items():\n",
    "            for competitor_base_model in df[\"base_model\"].unique():\n",
    "                competitor_model_name = (\n",
    "                    f\"{get_display_name(competitor_base_model)} ({perf_type})\"\n",
    "                )\n",
    "                win_loss_data[competitor_model_name] = {\n",
    "                    m: {\"win\": 0, \"tie\": 0, \"loss\": 0} for m in macro_cols\n",
    "                }\n",
    "\n",
    "                competitor_data = df[df[\"base_model\"] == competitor_base_model]\n",
    "\n",
    "                comparison_df = pd.merge(\n",
    "                    baseline_data,\n",
    "                    competitor_data,\n",
    "                    on=\"image_id\",\n",
    "                    suffixes=(\"_base\", \"_comp\"),\n",
    "                )\n",
    "                if comparison_df.empty:\n",
    "                    continue\n",
    "\n",
    "                for macro in macro_cols:\n",
    "                    base_error = comparison_df[f\"{macro}_base\"]\n",
    "                    comp_error = comparison_df[f\"{macro}_comp\"]\n",
    "                    win_loss_data[competitor_model_name][macro][\"win\"] = (\n",
    "                        base_error < comp_error\n",
    "                    ).sum()\n",
    "                    win_loss_data[competitor_model_name][macro][\"tie\"] = (\n",
    "                        base_error == comp_error\n",
    "                    ).sum()\n",
    "                    win_loss_data[competitor_model_name][macro][\"loss\"] = (\n",
    "                        base_error > comp_error\n",
    "                    ).sum()\n",
    "\n",
    "        if not win_loss_data:\n",
    "            print(\"No common images found to compare models.\")\n",
    "            return\n",
    "\n",
    "        competitor_models = sorted(win_loss_data.keys())\n",
    "        colors = {\"win\": \"#16A085\", \"tie\": \"#95A5A6\", \"loss\": \"#E74C3C\"}\n",
    "        macro_titles = [\"Calories\", \"Carbohydrates\", \"Protein\", \"Fat\"]\n",
    "        fig = make_subplots(\n",
    "            rows=len(competitor_models),\n",
    "            cols=4,\n",
    "            subplot_titles=macro_titles,\n",
    "            shared_xaxes=True,\n",
    "            vertical_spacing=0.08,\n",
    "            horizontal_spacing=0.08,\n",
    "        )\n",
    "\n",
    "        for i, competitor in enumerate(competitor_models):\n",
    "            row_idx = i + 1\n",
    "\n",
    "            for j, macro in enumerate(macro_cols):\n",
    "                col_idx = j + 1\n",
    "                data = win_loss_data[competitor][macro]\n",
    "                total = sum(data.values())\n",
    "                if total == 0:\n",
    "                    continue\n",
    "                win_pct, tie_pct, loss_pct = (\n",
    "                    data[\"win\"] / total * 100,\n",
    "                    data[\"tie\"] / total * 100,\n",
    "                    data[\"loss\"] / total * 100,\n",
    "                )\n",
    "                fig.add_trace(\n",
    "                    go.Bar(\n",
    "                        name=\"Win\",\n",
    "                        x=[win_pct],\n",
    "                        y=[competitor],\n",
    "                        orientation=\"h\",\n",
    "                        marker=dict(color=colors[\"win\"]),\n",
    "                        showlegend=False,\n",
    "                        text=f\"{win_pct:.0f}%\" if win_pct > 5 else \"\",\n",
    "                        textposition=\"inside\",\n",
    "                        customdata=[data[\"win\"]],\n",
    "                        hovertemplate=\"<b>Win</b><br>Count: %{customdata}<br>Percentage: %{x:.1f}%<br><extra></extra>\",\n",
    "                    ),\n",
    "                    row=row_idx,\n",
    "                    col=col_idx,\n",
    "                )\n",
    "                fig.add_trace(\n",
    "                    go.Bar(\n",
    "                        name=\"Tie\",\n",
    "                        x=[tie_pct],\n",
    "                        y=[competitor],\n",
    "                        orientation=\"h\",\n",
    "                        marker=dict(color=colors[\"tie\"]),\n",
    "                        showlegend=False,\n",
    "                        text=f\"{tie_pct:.0f}%\" if tie_pct > 5 else \"\",\n",
    "                        textposition=\"inside\",\n",
    "                        customdata=[data[\"tie\"]],\n",
    "                        hovertemplate=\"<b>Tie</b><br>Count: %{customdata}<br>Percentage: %{x:.1f}%<br><extra></extra>\",\n",
    "                    ),\n",
    "                    row=row_idx,\n",
    "                    col=col_idx,\n",
    "                )\n",
    "                fig.add_trace(\n",
    "                    go.Bar(\n",
    "                        name=\"Loss\",\n",
    "                        x=[loss_pct],\n",
    "                        y=[competitor],\n",
    "                        orientation=\"h\",\n",
    "                        marker=dict(color=colors[\"loss\"]),\n",
    "                        showlegend=False,\n",
    "                        text=f\"{loss_pct:.0f}%\" if loss_pct > 5 else \"\",\n",
    "                        textposition=\"inside\",\n",
    "                        customdata=[data[\"loss\"]],\n",
    "                        hovertemplate=\"<b>Loss</b><br>Count: %{customdata}<br>Percentage: %{x:.1f}%<br><extra></extra>\",\n",
    "                    ),\n",
    "                    row=row_idx,\n",
    "                    col=col_idx,\n",
    "                )\n",
    "                fig.update_yaxes(\n",
    "                    row=row_idx, col=col_idx, showticklabels=False, title_text=\"\"\n",
    "                )\n",
    "\n",
    "            pretty_base = get_display_name(baseline_model)\n",
    "            pretty_comp = competitor\n",
    "            axis_num = i * 4 + 1\n",
    "            y_anchor_ref = f\"y{'' if axis_num == 1 else axis_num}\"\n",
    "\n",
    "            fig.add_annotation(\n",
    "                x=-0.05,\n",
    "                y=competitor,\n",
    "                xref=\"paper\",\n",
    "                yref=y_anchor_ref,\n",
    "                text=f\"<b>{pretty_base}</b>\",\n",
    "                showarrow=False,\n",
    "                xanchor=\"right\",\n",
    "                yanchor=\"middle\",\n",
    "                font=dict(size=12),\n",
    "            )\n",
    "            fig.add_annotation(\n",
    "                x=1.05,\n",
    "                y=competitor,\n",
    "                xref=\"paper\",\n",
    "                yref=y_anchor_ref,\n",
    "                text=f\"<b>{pretty_comp}</b>\",\n",
    "                showarrow=False,\n",
    "                xanchor=\"left\",\n",
    "                yanchor=\"middle\",\n",
    "                font=dict(size=12),\n",
    "            )\n",
    "\n",
    "        fig.update_layout(\n",
    "            barmode=\"stack\",\n",
    "            title={\n",
    "                \"text\": f\"<b>{get_display_name(baseline_model)} vs. Others</b>\",\n",
    "                \"x\": 0.5,\n",
    "                \"xanchor\": \"center\",\n",
    "            },\n",
    "            height=max(200, 60 * len(competitor_models) + 80),\n",
    "            width=1000,\n",
    "            showlegend=False,\n",
    "            plot_bgcolor=\"rgba(250,251,252,0.8)\",\n",
    "            paper_bgcolor=\"white\",\n",
    "            margin=dict(l=200, r=200, t=100, b=20),\n",
    "        )\n",
    "        fig.update_xaxes(range=[0, 100], ticksuffix=\"%\", showticklabels=False)\n",
    "        fig.update_yaxes(showticklabels=False)\n",
    "\n",
    "        fig.show()\n",
    "\n",
    "    def plot_overall_score(self):\n",
    "        \"\"\"Create a bar chart comparing the overall score of each model.\"\"\"\n",
    "        if self.plot_df.empty or \"overall_score\" not in self.plot_df.columns:\n",
    "            print(\"No overall score data to plot.\")\n",
    "            return\n",
    "\n",
    "        summary_df = self.plot_df.groupby(\"model\")[\"overall_score\"].mean().reset_index()\n",
    "        summary_df = summary_df.sort_values(\"overall_score\", ascending=False)\n",
    "\n",
    "        fig = px.bar(\n",
    "            summary_df,\n",
    "            x=\"model\",\n",
    "            y=\"overall_score\",\n",
    "            title=\"<b>Overall Score</b>\",\n",
    "            labels={\"model\": \"Model\", \"overall_score\": \"Composite Score (0-100)\"},\n",
    "            text=\"overall_score\",\n",
    "            color=\"model\",\n",
    "            color_discrete_sequence=px.colors.qualitative.Plotly,\n",
    "        )\n",
    "        fig.update_traces(texttemplate=\"%{text:.2f}\", textposition=\"outside\")\n",
    "\n",
    "        max_score = summary_df[\"overall_score\"].max() if not summary_df.empty else 100\n",
    "\n",
    "        fig.update_layout(\n",
    "            uniformtext_minsize=8,\n",
    "            uniformtext_mode=\"hide\",\n",
    "            xaxis_title=None,\n",
    "            xaxis_tickangle=-45,\n",
    "            showlegend=False,\n",
    "            title_x=0.5,\n",
    "            title_font=dict(size=20, family=\"Arial Black\"),\n",
    "            font=dict(family=\"Arial, sans-serif\", size=12),\n",
    "            yaxis_range=[0, max_score * 1.15],\n",
    "        )\n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6909d270",
   "metadata": {},
   "source": [
    "## Run Benchmark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82972d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "BENCHMARK_CONFIG = {\n",
    "    \"models\": [\n",
    "        \"january/food-vision-v1\",\n",
    "        \"gpt-4o-mini\",\n",
    "        \"gpt-4o\",\n",
    "        \"gemini/gemini-2.5-flash-preview-05-20\",\n",
    "        \"gemini/gemini-2.5-pro-preview-06-05\",\n",
    "    ],\n",
    "    \"max_items\": 20,\n",
    "    \"cache_dir\": Path(\".cache/food_scan_bench\"),\n",
    "    \"max_concurrent_requests\": 50,\n",
    "    \"use_embeddings_for_matching\": True,\n",
    "    \"report_filename\": \"benchmark_results.csv\",\n",
    "}\n",
    "\n",
    "results_df = await run_evaluation(\n",
    "    models=BENCHMARK_CONFIG[\"models\"],\n",
    "    cache_dir=BENCHMARK_CONFIG[\"cache_dir\"],\n",
    "    max_items=BENCHMARK_CONFIG[\"max_items\"],\n",
    "    max_concurrent=BENCHMARK_CONFIG[\"max_concurrent_requests\"],\n",
    "    use_embeddings=BENCHMARK_CONFIG[\"use_embeddings_for_matching\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f63a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = BenchmarkAnalyzer(results_df)\n",
    "\n",
    "analyzer.summary_statistics()\n",
    "analyzer.create_performance_dashboard()\n",
    "analyzer.create_win_loss_analysis(baseline_model_name=\"january/food-vision-v1\")\n",
    "analyzer.plot_overall_score()\n",
    "\n",
    "# analyzer.export_detailed_report(BENCHMARK_CONFIG[\"report_filename\"])\n",
    "\n",
    "# display(results_df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
